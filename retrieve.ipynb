{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leon/.local/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at Narrativa/legal-longformer-base-4096-spanish and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(52000, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(4098, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"Narrativa/legal-longformer-base-4096-spanish\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create index with FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the document embeddings\n",
    "embeddings_df = pd.read_csv('corpus/corpus_embeddings.csv')  # Replace with your path\n",
    "document_embeddings = embeddings_df.values  # Convert to numpy array\n",
    "\n",
    "# Normalize document embeddings\n",
    "document_embeddings = document_embeddings / np.linalg.norm(document_embeddings, axis=1, keepdims=True)\n",
    "\n",
    "# Convert to C-contiguous format\n",
    "document_embeddings = np.ascontiguousarray(document_embeddings, dtype=np.float32)\n",
    "\n",
    "# Create Faiss index\n",
    "index = faiss.IndexFlatIP(document_embeddings.shape[1])  # IP = Inner Product (dot product)\n",
    "index.add(document_embeddings)  # Add the document embeddings to the index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to get embedding for text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get query embedding\n",
    "def get_embedding(text, model, tokenizer, device):\n",
    "    inputs = tokenizer(text, return_tensors='pt', max_length=4096, truncation=True, padding=True)\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state[:, 0, :].cpu().numpy().reshape(-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get query embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example query\n",
    "query = \"SR. HANS FRIEDICH SCHUCHARDT\"  # Replace with your query\n",
    "\n",
    "# Get query embedding\n",
    "query_embedding = get_embedding(query, model, tokenizer, device)\n",
    "\n",
    "# Normalize the query embedding\n",
    "query_embedding = query_embedding / np.linalg.norm(query_embedding)\n",
    "\n",
    "# Convert to 2D numpy array as required by Faiss\n",
    "query_embedding = np.expand_dims(query_embedding, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search and retrieve top k documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 similar documents (indices): [2276 3679 1979 2727 4854]\n",
      "Similarity scores: [0.6976906  0.57066333 0.55561125 0.5490911  0.54366434]\n"
     ]
    }
   ],
   "source": [
    "# Search for the top 5 most similar documents\n",
    "D, I = index.search(query_embedding, k=5)  # k is the number of top results to return\n",
    "\n",
    "# Print results\n",
    "print(\"Top 5 similar documents (indices):\", I[0])\n",
    "print(\"Similarity scores:\", D[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your original dataset\n",
    "df = pd.read_csv('corpus/corpus.csv')  # Replace with your actual dataset path\n",
    "\n",
    "# Extract document embeddings from the 'text' column\n",
    "texts = df['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 similar documents (Codigo values): [40300, 87051, 78677, 72793, 40790]\n"
     ]
    }
   ],
   "source": [
    "# Example: Let's assume these are the top indices retrieved from the similarity search\n",
    "top_indices = I[0]\n",
    "\n",
    "# Extract the \"Codigo\" column into a list or another DataFrame\n",
    "codigos = df['Codigo'].tolist()  # This will create a list of 'Codigo' values corresponding to the original data\n",
    "\n",
    "# Map the retrieved indices to \"Codigo\" values\n",
    "top_codigos = [codigos[idx] for idx in top_indices]\n",
    "\n",
    "# Print the mapped \"Codigo\" values\n",
    "print(\"Top 5 similar documents (Codigo values):\", top_codigos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXPEDIENTE: RECURSO EXTRAORDINARIO DE\n",
      "CORTE REVISION INTERPUFSTO POR FL SR. CELSO\n",
      "SUPREMA RODRIGUEZ CABELLO POR DERECHO PROPIO Y\n",
      "peJUSTICIA BAJO PATROCINIO DEL ABG. ANDRES RAMON\n",
      "RECALDE EN EL EXPTE MINISTERIO PUBLICO C \n",
      "CELSO RODRIGUEZ CABELLO S S.H.P. C LA\n",
      "A PROPIEDAD - ESTAFA , w ene eneeoneesneesweenneeneeeneesneene\n",
      " \n",
      "a ) ACUERDO Y SENTENCIA i ata y Jiele. \n",
      "la ciudad de Asuncion, capital de la Republica del Paraguay, alos ... UY...\n",
      "dias del mes de Novien! .. del afio dos mil veintiuno, estando presentes en la Sala de\n",
      "Acuerdos, los sefiores ministros de la Excma. Corte Suprema de Justicia, Sala Penal, Doctores\n",
      "LUIS MARIA BENITEZ RIERA, MANUEL DEJESUS RAMIREZ CANDIA y MARIA\n",
      "CAROLINA LLANES, ante mi la Secretaria Autorizante, se trajo a estudio el expediente\n",
      "caratulado: RECURSO EXTRAORDINARIO DE REVISION INTERPUESTO POR EL\n",
      "SR. CELSO RODRIGUEZ CABELLO POR DERECHO PROPIO Y BAJO PATROCINIO\n",
      "DEL ABG. ANDRES RAMON RECALDE EN EL EXPTE MINISTERIO PUBLICO C \n",
      "CELSO RODRIGUEZ CABELLO S S.H.P. C LA PROPIEDAD - ESTAFA , a los efectos\n",
      "de resolver el Recurso de Revisién interpuesto contra la S.D N 501 de fecha 01 de diciembre\n",
      "de 2016, dictado por el Tribunal de Sentencias de la Capital, integrado por los jueces Victor\n",
      "Manuel Medina, Digno Arnaldo Fleitas Ortiz y Elsa Maria Garcia Hulskamp. \n",
      "Previo estudio de los antecedentes del caso, la Excelentisima Corte Suprema de Justicia,\n",
      "resolvié plantear las siguientes: ncnononnnnnnneeeeee cnc nnnnnnnenennnenee\n",
      "CUESTIONES:\n",
      " Es admisible el recurso de revisién interpuesto? neenenewnnene\n",
      "En su caso resulta procedente? anna nnn nnn\n",
      "Practicado el sorteo de ley para determinar el orden de votacidn, arrojé el siguiente\n",
      "resultado: Dra. Maria Carolina Llanes, Dr. Luis Benitez Riera y Dr. Manuel Dejestis Ramirez\n",
      "Candia, 2-2-2 n-nonane nnn nn nnn nnn nnn ence neem nnn nnnnnnnnnnne\n",
      "A la primera cuestién planteada, la ministra Dra. Maria Carolina Llanes, dijo: En\n",
      "primer término corresponde realizar el examen de admisibilidad del recurso interpuesto a tenor de\n",
      "lo dispuesto en los Arts. 481, 482 y 483 del Cédigo Procesal Penal, conforme se expone a\n",
      "continuacién a) Objeto impugnado: El fallo atacado constituye una Sentencia Definitiva\n",
      "di tado por un Tribunal de Sentencias que \n"
     ]
    }
   ],
   "source": [
    "# get text for a specific Codigo\n",
    "text = df.loc[df[\"Codigo\"] == 87051, \"text\"].values[0]\n",
    "print(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
